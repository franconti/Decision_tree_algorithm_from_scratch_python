{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.\n",
        "\n",
        "The model starts with a root node, which does not have any incoming branches. The outgoing branches from the root node then feed into the internal nodes, also known as decision nodes. Based on the available features, both node types conduct evaluations to form homogenous subsets, which are denoted by leaf nodes, or terminal nodes. The leaf nodes represent all the possible outcomes within the dataset.\n",
        "Decision tree learning employs a divide and conquer strategy by conducting a greedy search to identify the optimal split points within a tree. This process of splitting is then repeated in a top-down, recursive manner until all, or the majority of records have been classified under specific class labels.\n",
        "\n",
        "Two methods are populary used to choose the best attribute at each node, Entropy and Gini impurity.\n",
        "\n",
        "Entropy measures the impurity of the sample values. Its  values can fall between 0 and 1. If all samples in data set, S, belong to one class, then entropy will equal zero. If half of the samples are classified as one class and the other half are in another class, entropy will be at its highest at 1.\n",
        "\n",
        "In order to select the best feature to split on and find the optimal decision tree, the attribute with the smallest amount of entropy should be used. Information gain represents the difference in entropy before and after a split on a given attribute. The attribute with the highest information gain will produce the best split as it is doing the best job at classifying the training data according to its target classification.\n",
        "\n",
        "Secondly, Gini impurity is the probability of incorrectly classifying random data point in the dataset if it were labeled based on the class distribution of the dataset.\n",
        "\n",
        "\n",
        "To make these concepts work and create a decision tree algorithm from scratch, this code is arranged as follows:\n",
        "\n",
        "1-  Determines if the tree is for regresion or classification.\n",
        "\n",
        "2 - Identifies potential split values for each feature, considering both categorical and continuous data types.\n",
        "\n",
        "3- Computes evaluation metrics like Gini impurity or entropy for classification trees and variance for regression trees to assess the quality of potential splits.\n",
        "\n",
        "4- Splits the node based on a specific feature and its value, creating left and right child nodes. Recursively splits nodes until a stopping criterion (depth or score threshold) is met, forming the decision tree.\n",
        "\n",
        "\n",
        "###References:\n",
        "\n",
        "\n",
        "https://www.ibm.com/topics/decision-trees#:~:text=A%20decision%20tree%20is%20a,internal%20nodes%20and%20leaf%20nodes.\n",
        "\n",
        "https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/"
      ],
      "metadata": {
        "id": "XCHWy5wyz9n9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "1) ID number\n",
        "2) Diagnosis (M = malignant, B = benign)\n",
        "3-32)\n",
        "\n",
        "Ten real-valued features are computed for each cell nucleus:\n",
        "\n",
        "3) radius (mean of distances from center to points on the perimeter)\n",
        "\n",
        "4) texture (standard deviation of gray-scale values)\n",
        "\n",
        "5) perimeter\n",
        "\n",
        "6) area\n",
        "\n",
        "7) smoothness (local variation in radius lengths)\n",
        "\n",
        "8) compactness (perimeter^2 / area - 1.0)\n",
        "\n",
        "9) concavity (severity of concave portions of the contour)\n",
        "\n",
        "10) concave points (number of concave portions of the contour)\n",
        "\n",
        "11) symmetry\n",
        "\n",
        "12) fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "\n",
        "The mean, standard error and \"worst\" or largest (mean of the three\n",
        "largest values) of these features were computed for each image,\n",
        "resulting in 30 features. For instance, field 3 is Mean Radius, field\n",
        "13 is Radius SE, field 23 is Worst Radius.\n",
        "\n",
        "All feature values are recoded with four significant digits.\n",
        "\n",
        "Missing attribute values: none\n",
        "\n",
        "Class distribution: 357 benign, 212 malignant\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###Reference:\n",
        "http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29"
      ],
      "metadata": {
        "id": "7X7nfr_7bn84"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RP1vUP3OPdUQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from more_itertools import set_partitions\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "import more_itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DecisionTreeNode:\n",
        "\n",
        "    def __init__(self,features_mat,y_labels,depth,metrics_type='Gini',score_to_stop_split=0.9):\n",
        "        self.right_node = None\n",
        "        self.left_node = None\n",
        "        self.feature_index_split = None\n",
        "        self.value_split = None\n",
        "        self.depth_in_tree = depth\n",
        "        self.features_mat = features_mat\n",
        "        self.y_labels = y_labels\n",
        "        self.label_of_leaf = None\n",
        "        self.metrics_type = metrics_type\n",
        "        self.score_to_stop_split = score_to_stop_split\n",
        "        self.is_classification_tree = self._check_if_is_classification_tree()\n",
        "        self.is_trained = False\n",
        "\n",
        "    def _check_if_is_classification_tree(self):\n",
        "       #This fucntion evaluates if it a clasification or regression\n",
        "        return True if self.y_labels.dtype==object else False\n",
        "\n",
        "    def _split_one_node(self,feature_index,value_to_split):\n",
        "        if type(value_to_split)==list:  #for categorical features\n",
        "            first_group = value_to_split[0] #takes the first value\n",
        "            mask = np.isin(self.features_mat[:,feature_index],first_group) #use np.isin to identify data points where the feature value belongs to the first category.\n",
        "        else:\n",
        "            mask = self.features_mat[:,feature_index]<=value_to_split #Array true and false. continues values\n",
        "        right_features_mat = self.features_mat[mask]\n",
        "        left_features_mat = self.features_mat[~mask]\n",
        "        right_labels = self.y_labels[mask]\n",
        "        left_labels = self.y_labels[~mask]\n",
        "        return left_features_mat,right_features_mat,right_labels,left_labels\n",
        "\n",
        "    def _get_features_to_check(self,num_features):\n",
        "      #return a sequence of numbers of columns\n",
        "        return range(num_features)\n",
        "\n",
        "    def _get_all_split_values(self,feature_index):\n",
        "      # Identifies and prepares all potential split points for a given feature in the tree algorithm, accounting for both categorical and continuous data types.\n",
        "\n",
        "        values = self.features_mat[:,feature_index] #takes all the values from one feature of the dataset\n",
        "        unique_values = np.unique(values) #Extract the unique values\n",
        "\n",
        "        if type(values[0])==str: #If the feature is categorical\n",
        "            split_values = list(set_partitions(unique_values,2)) #it uses set_partitions to generate all possible combinations of splitting the unique values into two groups.\n",
        "\n",
        "        else: #If the feature is continuous (numeric type)\n",
        "            sorted_values = np.sort(unique_values)\n",
        "            if len(unique_values)<10000:\n",
        "                #Creates split points by taking the midpoint between adjacent values\n",
        "                sorted_values_remove_first = sorted_values[1:] #take values from second element to end\n",
        "                sorted_values_no_last = sorted_values[:-1] #from beguining witho out the last one.\n",
        "                split_values = (sorted_values_no_last+sorted_values_remove_first)/2\n",
        "            else:\n",
        "              #if the unique value exceeds 10,000, it divides the continuous range of values into percentile ranges from 1 to 99 percentiles, with a step of 1\n",
        "               split_values = np.percentile(sorted_values,range(1,99,1))\n",
        "        return split_values\n",
        "\n",
        "\n",
        "    def _calcualte_metrics(self,right_labels,left_labels):\n",
        "      #this function evaluates the quality of the potential splits using evaluation metrics such as \"Gini\" or \"Entropy\" for classification trees, and \"Variance\" for regression trees\n",
        "\n",
        "        is_gini = self.metrics_type == 'Gini' #defines metric\n",
        "        metrics = 0\n",
        "        total_size = len(left_labels) + len(right_labels) #get lenght\n",
        "\n",
        "        for branch_labels in [right_labels,left_labels]:\n",
        "\n",
        "            if self.is_classification_tree: #if it is a clasification tree\n",
        "                vals,counts = np.unique(branch_labels,return_counts=True) #get unique values and their corresponding counts\n",
        "                probabilities = counts/counts.sum() #calculates the probability of occurrence for each unique value\n",
        "                gini_of_branch = 1-(probabilities**2).sum() #it calculates the Gini impurity. A lower Gini impurity suggests a better separation of classes in that particular node of the tree\n",
        "                entropy_of_branch = -(probabilities*np.log2(probabilities)).sum() #calculate the entropy of a branch\n",
        "                metrics_branch = gini_of_branch if is_gini else entropy_of_branch\n",
        "            else:\n",
        "                metrics_branch = np.var(branch_labels) #for regression tree, it calculates the variance of the branch labels.\n",
        "\n",
        "            weight_of_branch = len(branch_labels)/total_size #Computes the weight of the current branch\n",
        "            metrics += metrics_branch*weight_of_branch #it sums the weighted metrics for both branches. it represents the overall quality of the potential split\n",
        "        return metrics\n",
        "\n",
        "    def _check_all_options_for_split(self):\n",
        "      #This function iterates through features and potential split values to find the best possible split point. Returns the datasets resulting from the best split\n",
        "\n",
        "\n",
        "        num_columns = self.features_mat.shape[1] #number of features\n",
        "        best_score = np.inf # positive infinity as initial best score\n",
        "        features_to_search = self._get_features_to_check(num_columns) #get range of features\n",
        "\n",
        "\n",
        "        for feature_index in features_to_search: #loop through features to search\n",
        "            split_values = self._get_all_split_values(feature_index) #get potential split\n",
        "\n",
        "            for value in split_values: #Loop through the potential split values\n",
        "                left_features_mat,right_features_mat,right_labels,left_labels = \\\n",
        "                                                self._split_one_node(feature_index,value)\n",
        "                score = self._calcualte_metrics(right_labels,left_labels) # Calculate metrics for the split\n",
        "\n",
        "                if score < best_score: # Check if the current split's score is better\n",
        "                    best_score = score\n",
        "                    best_feature = feature_index\n",
        "                    best_value = value\n",
        "                    best_right_features_mat = right_features_mat\n",
        "                    best_left_features_mat = left_features_mat\n",
        "                    best_left_labels = left_labels\n",
        "                    best_right_labels = right_labels\n",
        "        self.feature_index_split = best_feature\n",
        "        self.value_split = best_value\n",
        "        return best_right_features_mat,best_left_features_mat,best_left_labels,best_right_labels\n",
        "\n",
        "    def _calc_score_and_label_of_node(self):\n",
        "      #compute the score and prediction label for a node in a decision tree. It returns the calculated score and prediction label\n",
        "\n",
        "        if self.is_classification_tree: #for clasification\n",
        "            vals,freq = np.unique(self.y_labels,return_counts=True)   #calculates unique values and their frequencies\n",
        "\n",
        "            #calculating purity\n",
        "            probabilities = freq/freq.sum()  #probabilities of each unique label\n",
        "            max_p = np.max(probabilities) #Finds the maximum probability among all classes.\n",
        "            score = max_p\n",
        "\n",
        "            #calculating label\n",
        "            max_index = np.argmax(freq) #Obtains the index of the most frequent class.\n",
        "            prediction = vals[max_index] #Assigns the label associated with the most frequent class as the prediction label.\n",
        "\n",
        "        else: #For a Regression Tree:\n",
        "\n",
        "            minus_var = -np.var(self.y_labels) #Calculates the negative variance of the target labels. Higher magnitude means more spread\n",
        "            score = minus_var\n",
        "            prediction = np.median(self.y_labels) #Computes the median of the target labels\n",
        "        return score,prediction\n",
        "\n",
        "    def _split_node_recursively(self,max_depth=6):\n",
        "        # This function builds the decision tree recursively by splitting nodes until a stopping condition is met, at which point the node becomes a leaf by storing the final label value.\n",
        "\n",
        "        self.is_trained = True #Marks the current node as trained.\n",
        "        score,label_of_node = self._calc_score_and_label_of_node()\n",
        "\n",
        "        #stopping criteria for tree\n",
        "        if self.depth_in_tree > max_depth or score > self.score_to_stop_split: #Checks if the current node's depth exceeds the max_depth allowed\n",
        "            self.label_of_leaf = label_of_node  #this is a leaf\n",
        "            del self.features_mat,self.y_labels #Deletes the features and labels associated with this node\n",
        "\n",
        "        else:\n",
        "            right_features_mat,left_features_mat,left_labels,right_labels = \\\n",
        "                                                self._check_all_options_for_split()\n",
        "            del self.features_mat,self.y_labels\n",
        "\n",
        "            self.right_node = DecisionTreeNode(right_features_mat,right_labels,self.depth_in_tree+1,\\\n",
        "                    self.metrics_type,self.score_to_stop_split) #Initializes the right child node\n",
        "            self.right_node._split_node_recursively(max_depth)\n",
        "\n",
        "\n",
        "            self.left_node = DecisionTreeNode(left_features_mat,left_labels,self.depth_in_tree+1,\\\n",
        "                    self.metrics_type,self.score_to_stop_split) #Initializes the left child node\n",
        "            self.left_node._split_node_recursively(max_depth)\n",
        "\n",
        "    def fit(self,max_depth = 6):\n",
        "        self._split_node_recursively(max_depth)\n",
        "\n",
        "\n",
        "    def predict_numerical_features_only(self,row):\n",
        "       if self.label != None:\n",
        "           return self.label\n",
        "       else:\n",
        "           if row[self.feature_index] > self.value:\n",
        "               return self.left.predict(row)\n",
        "           else:\n",
        "               return self.right.predict(row)\n",
        "\n",
        "    def predict(self,data_row):\n",
        "      #function to make predictions\n",
        "       if not self.is_trained:\n",
        "          print(\"Model not trained yet\")\n",
        "          return\n",
        "\n",
        "       #stoppint criteria - if we are in a leaf\n",
        "       if self.label_of_leaf != None:\n",
        "           return self.label_of_leaf\n",
        "       else:\n",
        "\n",
        "           # categorical or numerical features\n",
        "           if type(self.value_split)==list: #if the categorical alue is in the list\n",
        "\n",
        "              if data_row[self.feature_index_split].isin(self.value_split[0]):\n",
        "                    return self.left_node.predict(data_row)\n",
        "              else:\n",
        "                   return self.right_node.predict(data_row)\n",
        "\n",
        "           else: #for numerical features\n",
        "               if data_row[self.feature_index_split] > self.value_split:\n",
        "                   return self.left_node.predict(data_row)\n",
        "               else:\n",
        "                   return self.right_node.predict(data_row)\n",
        "\n",
        "    def print_tree(self):\n",
        "        if self.label_of_leaf!=None:\n",
        "            print('\\t'*(self.depth_in_tree-1),'label is: ',self.label_of_leaf,\\\n",
        "                  'and depth is: ',self.depth_in_tree)\n",
        "        else:\n",
        "            print('\\t'*(self.depth_in_tree-1),'split is at feature: ',\\\n",
        "                      self.feature_index_split,' and value ',self.value_split,\\\n",
        "                      ' and depth is: ',self.depth_in_tree)\n",
        "        # if self.right_node:\n",
        "            self.right_node.print_tree()\n",
        "        # if self.left_node:\n",
        "            self.left_node.print_tree()\n",
        "\n",
        "    def save_tree(self,file_name = 'my_tree'):\n",
        "        if not self.is_trained:\n",
        "            print(\"Tree isn't trained yet\")\n",
        "        else:\n",
        "            with open(file_name,'wb') as fid:\n",
        "                pickle.dump(self,fid)\n",
        "\n",
        "\n",
        "    def load_tree(file_name = 'my_tree'):\n",
        "        with open(file_name,'rb') as fid:\n",
        "            tree = pickle.load(fid)\n",
        "        return tree\n",
        "\n",
        "    def evalute(self,test_x,test_y):\n",
        "      #gets accuracy\n",
        "        test_labels = np.array([self.predict(row_data) for row_data in test_x])\n",
        "        if self.is_classification_tree:\n",
        "            metrics = 100*(test_labels==test_y).sum()/len(y_test)\n",
        "        else:\n",
        "            metrics = np.sqrt(np.mean( (test_labels-test_y)**2 ))\n",
        "        return metrics\n"
      ],
      "metadata": {
        "id": "jmKKtIUTYX9L"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    #reading data and pre-processing\n",
        "    file_name = r'/content/drive/MyDrive/Colab Notebooks/wdbc.data'\n",
        "    data = pd.read_csv(file_name,header = None)\n",
        "\n",
        "    y = data.iloc[:,1].values #target\n",
        "    x = data.drop(1,axis=1) # features\n",
        "    x = x.drop(0,axis=1).values #remove id from data\n",
        "    x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.8) #split\n",
        "\n",
        "    root_node = DecisionTreeNode(x_train,y_train,1)\n",
        "    root_node.fit(5)\n",
        "    root_node.print_tree()\n",
        "    accuracy = root_node.evalute(x_test,y_test)\n",
        "    root_node.save_tree()\n",
        "    print('\\naccuracy is {}'.format(accuracy))\n",
        "\n",
        "    new_tree = DecisionTreeNode.load_tree()\n",
        "    prediction_loaded = new_tree.predict(x_test[0,:])\n",
        "\n",
        "    sklern_tree = DecisionTreeClassifier(max_depth=5)\n",
        "    sklern_tree.fit(x_train,y_train)\n",
        "    sklearn_score = sklern_tree.score(x_test,y_test)\n",
        "    print('\\nsklearn accuracy is {}'.format(sklearn_score))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buopnzl5dTnq",
        "outputId": "c4946579-9d2f-425e-b1ca-c2410c114021"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " split is at feature:  22  and value  105.95  and depth is:  1\n",
            "\t label is:  B and depth is:  2\n",
            "\t split is at feature:  21  and value  20.645  and depth is:  2\n",
            "\t\t split is at feature:  7  and value  0.08365500000000001  and depth is:  3\n",
            "\t\t\t label is:  B and depth is:  4\n",
            "\t\t\t label is:  M and depth is:  4\n",
            "\t\t label is:  M and depth is:  3\n",
            "\n",
            "accuracy is 90.35087719298245\n",
            "\n",
            "sklearn accuracy is 0.9385964912280702\n"
          ]
        }
      ]
    }
  ]
}